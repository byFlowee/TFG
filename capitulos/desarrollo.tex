%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Plantilla TFG/TFM
% Escuela PolitÃ©cnica Superior de la Universidad de Alicante
% Realizado por: Jose Manuel Requena Plens
% Contacto: info@jmrplens.com / Telegram:@jmrplens
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Desarrollo}
\label{desarrollo}
\textit{This chapter develops the main work of this thesis and is organized as follows. Section \ref{sec:expanding} will describe the process of automating the UnrealROX Actor. Section \ref{sec:tracker} will go through the process of recording sequences. Finally, Section \ref{sec:segnet} will cover all the deep learning work such as the network implementation and the data pre-processing.  }

\section{Expanding the UnrealROX Framework}
\label{sec:expanding}
As we previously mentioned in section \ref{introduction} one of the main goals on this work is to expand the UnrealROX framework in order to automatize the generation of synthetic data without the need of a VR Headset and user input. In this section we will further detail the framework itself along with the data generation process.

UnrealROX can automatically generate and annotate data from a recorded sequence, but manually recording can be tedious and time consuming. In this work we have built the basic framework for the programmer to include its own actions and execute them in a sequential way, much like other frameworks such as VirtualHome \cite{virtualhome2018}. 

\subsection{The ROXBasePawn Class}
This is the main class that contains all the logic for the character controller (movement, animations, grasping) of any robot pawn. It allows for the user introduce a robot to the scene and manually move and interact with the objects in a scene. We will use this class as our parent for the implementation.

\subsection{The ROXBotPawn Class}
The ROXBotPawn class inherits from ROXBasePawn and will handle all the logic for the automation of tasks of any \textit{Pawn} within a scene. In order to model all the different actions and interactions the Enum $EActionType$ was created, here the programmer can add any type of action to be built into the system.

Also in order to model the actions themselves, the $FROXAction$ struct was built, containing a pointer to the target, as well as the type of action $EActionType$, the structure is shown in listing \ref{code:actionStruct}.

\begin{lstlisting}[style=C++, caption=FROXAction struct, frame=single, label=code:actionStruct]
	USTRUCT(BlueprintType)
	struct FROXAction
	{
		GENERATED_USTRUCT_BODY()
		UPROPERTY(BlueprintReadWrite, EditAnywhere, Category = Pathfinding)
		AActor* target;
		
		UPROPERTY(BlueprintReadWrite, EditAnywhere, Category = Pathfinding)
		EActionType action;
		
		FROXAction() : target(nullptr), action(EActionType::MoveTo) {};
		FROXAction(AActor* tg, EActionType t) : target(tg), action(t) {};
	}
\end{lstlisting}

In order for the programmer to add actions and queue them from the \gls{ue4} editor we built the $doAction(AActor*, EActionType)$ (seen in listing \ref{code:queueAction}) and made it BlueprintCallable, this way, in a simple way, actions can be queued from the editor and the $Pawn$ will execute them in a sequential order as seen in Figure \ref{action_queue}. The target actor can be picked from the editor by making a new variable and the type of action can be selected with a drop down menu in the body of the blueprint function.

\begin{lstlisting}[style=C++, caption=doAction function which queues a new FROXAction to the system, frame=single, label=code:queueAction]
	UFUNCTION(BlueprintCallable)
	void AROXBotPawn::doAction(AActor* actor, EActionType type) 
	{	
		actions.Add(FROXAction(actor, type));
	}
\end{lstlisting}

\begin{figure}[h]
	\includegraphics[scale=0.4]{archivos/action_queue.png}
	\centering
	\caption{Example of queuing 3 "MoveTo" actions from the editor}
	\label{action_queue}
\end{figure}

The $doAction()$ method creates a $FROXActions$ and pushes it to the queue. Whenever the queue contains an action and the $Pawn$ is not executing one, it will run the $fetchNextAction()$ method. This will pop the action from the queue and execute the method corresponding to that action.

Additionally, pathfinding and movement logic was implemented in order to define the $MoveTo$ action. In a first approach, the MoveToLocation method of the default \gls{ue4} Actor class was used, but the idea was discarded since we needed to work with the ROXBasePawn instead of the default Actor. Another option that was studied was Environment Query Systems (EQS), which is an experimental feature within the \gls{ai} system in \gls{ue4} and allows to gather data from the environment. It was finally discarded since the complexity of the technology was greater than that of the task we needed to solve, this is due to the fact that we only need the position of a certain actor in order to travel towards them, and this can be achieved without the need of more complex objects. 

With the previous options discarded, we decided to go with a custom implementation of the movement. In order to accomplish this, the NavigationMesh component of \gls{ue4} was used along with the $FindPathToActorSynchronously$ method, which returns a $UNavigationPath$ containing all the path-points from one actor to another. Once we obtain the path-points the $VInterpConstantTo$ from the FMath library and the $RInterpTo\_Constant$ from the UKismeMathLibrary are used in order to obtain the next vector transformation for both position and rotation of the $Pawn$, this movement logic can be seen in listing \ref{code:movementLogic}. These methods interpolate the current location with the next path-point location in order to achieve a smooth transition and make the movement more natural.

\begin{lstlisting}[style=C++, caption=Movement logic for the pathfinding algorithm, frame=single, label=code:movementLogic]
	FVector nextPos = FMath::VInterpConstantTo(this->ActorLocation(), FVector(pathPoints[i].X, pathPoints[i].Y, ActorLocation().Z), DeltaTime, vel);
	FRotator nextRot = UKismetMathLibrary::RInterpTo_Constant(ActorRotation(), UKismetMathLibrary::FindLookAtRotation(ActorLocation(), nextPos), DeltaTime, rotateVel);
	SetActorRotation(nextRot);
	SetActorLocation(nextPos);
\end{lstlisting}

\subsection{Animating the ROXBotPawn}
As we previously mentioned in section \ref{sec:sim2real}, simulating the 3D environment with extreme detail is a must in order for the algorithms to properly infer the knowledge and transfer it to the real world. In this section we will take a look into the process of creating a new animation for the ROXBotPawn using \gls{ue4} blueprints.

The ROXBasePawn already provides a default walking animation for our Actor, however it is thought to work with a VR Headset, therefore, it takes into account the pose and movement of both hand controllers and the headset itself in order to move accordingly to the user. Since our ROXBotPawn does not require such data, we will change the blueprint animation in order to fit our needs. 

First of all, we need the assets, in our case we will be using the default walk and idle animation from the UnrealROX framework. In order to have smooth transitions between different animations i.e from idle to walking, we have used \gls{ue4} BlendSpaces, which are special assets that can be sampled in the Animation Graph and allow for blended transitions based on one or more inputs. In our case,  we will blend the animation based on the speed of the Bot, a preview of said asset can be seen in Figure \ref{fig:blend_space}.

\begin{figure}[h]
	\includegraphics[width=0.5\textwidth]{archivos/blend_space.png}
	\centering
	\caption{Blend Space asset which samples the transition from idle walking state animation, 0 speed would translate into a complete idle, while 90 would be walking forward.}
	\label{fig:blend_space}
\end{figure}

In order to use the BlendSpaces, we need to create our Event Graph and compute the Bot speed, we do this by obtaining its position in the current and last tick, therefore obtaining the travel distance in one tick. We can now apply the dot operator with the forward vector and divide by the delta time, obtaining the current speed, the complete logic of this function as well as the full Event Graph can be seen in Figures \ref{fig:event_graph} and \ref{fig:speed_calc}.

\begin{figure}[h]
	\includegraphics[width=\textwidth]{archivos/animation_event_graph.png}
	\centering
	\caption{Animation event graph which will obtain the Bot data, the SpeedCalc module is expanded in Figure \ref{fig:speed_calc}.}
	\label{fig:event_graph}
\end{figure}

\begin{figure}[h]
	\includegraphics[width=\textwidth]{archivos/speed_calc.png}
	\centering
	\caption{Blueprint sub-module which computes the instantaneous speed of the Bot.}
	\label{fig:speed_calc}
\end{figure}

With the event graph and the BlendSpaces created all that is left to do is create the state machine which will contain the state and transition logic of the different animations. In our case, we just need an idle, walk forward and walk backwards states, this state machine is shown in Figure \ref{fig:state_machine}.

\begin{figure}[h]
	\includegraphics[scale=0.6]{archivos/animation_state_machine.png}
	\centering
	\caption{Animation state machine with idle, walk forward and walk backwards states and the transitions logic.}
	\label{fig:state_machine}
\end{figure}

The logic for the transitions between states are very simple, we will simply check if the speed surpasses a certain threshold, for instance, if the $Speed Anim$ is greater than 0.1, we will transition from the $Idle$ state to the $WalkForward$, in the same manner, when the speed value falls under negative $0.1$, we will transition towards the $WalkBackwards$ state.

\section{Recording sequences with UnrealROX}
\label{sec:tracker}
In the previous section, we developed the process of automatization of the framework in order to record sequences without the need of manually doing the actions with a VR Headset. In this section we will go through the ROXTracker class and how to use it in order to record sequences (subsection \ref{sec:recording}) as well as how to play them and generate the data (subsection \ref{sec:playback}).

The ROXTracker is an empty actor that has knowledge of the whole scene. In order to use it, we just need to search for it in the contextual menu (as shown in Figure \ref{fig:tracker_object}) and drag it to our scene. While in record mode, the ROXTracker will be able to store all of the information needed in order to rebuild the sequence offline as a TXT or JSON file, this will allow then to run the sequence in playback mode and generate frame by frame, ground truth annotated images such as the segmentation masks, depth and normal maps, as well as the RGB images.

\begin{figure}[h]
	\includegraphics[width=\textwidth]{archivos/tracker_object.png}
	\centering
	\caption{ROXTracker Object in the \gls{ue4} contextual menu.}
	\label{fig:tracker_object}
\end{figure}

Once we have our ROXTracker in the World Outliner, we can tweak its behavior and change certain settings, some of the most important ones are listed as follows:

\begin{itemize}
	\item \textbf{Record Mode:} When checked, the ROXTracker will operate in record mode, this means that if the user presses the record key, it will start gathering and writing all of the necessary data to a TXT file.
	\item \textbf{Scene Save Directory:} As its own name implies, this variable stores the path where the data will be saved.
	\item \textbf{Scene Folder:}  Name of the folder inside the Scene Save Directory path where all the data files will be stored.
	\item \textbf{Generate Sequence Json:} When pressed will look for a TXT file inside the Secene Folder with name corresponding to the field \textbf{Input Scene TXT File Name} and will generate its equivalent JSON file with the name on the field \textbf{Output Scene Json File Name} i.e looking at Figure \ref{fig:tracker_settings}, the Tracker will search for a $scene.txt$ file inside the $unrealrox/RecordedSequences$ folder and generate a $scene.json$ file.
\end{itemize}

\begin{figure}[h]
	\includegraphics[width=0.8\textwidth]{archivos/tracker_settings.png}
	\centering
	\caption{ROXTracker settings in the \gls{ue4} editor.}
	\label{fig:tracker_settings}
\end{figure}

\subsection{Recording mode}
\label{sec:recording}

Before start generating sequences we have to make some tweaks to the recording settings of the ROXTracker, which can be seen in Figure \ref{fig:recording_settings} and further explained as follows:

\begin{itemize}
	\item \textbf{Pawns:} Array that contains a reference to every actor that the user wants to keep track of.
	\item \textbf{Camera Actors:} Array containing the cameras that will be tracked.
	\item \textbf{Stereo Camera Baselines:} Array that stores the focal distance (baseline) between the corresponding camera in the \textbf{CameraActors} array. It can be left empty if there are none.
	\item \textbf{Scene File Name Prefix:} Every generated file of raw scene data will share this prefix in its filename.
\end{itemize}

\begin{figure}[h]
	\includegraphics[width=0.5\textwidth]{archivos/recording_settings.png}
	\centering
	\caption{ROXTracker recording settings.}
	\label{fig:recording_settings}
\end{figure}

Once all of the fields are filled with the desired actors and cameras to be tracked and the filenames are set we can start recording a sequence, to do this, we simply have to set the ROXTracker in record mode and run the scene by hitting play on the editor. To begin/stop recording the user needs press "R", a red \textbf{RECORDING} message will display at the top of the screen, an example of the recorder running can be seen inf Figure \ref{fig:recording}.

\begin{figure}[h]
	\includegraphics[width=0.6\textwidth]{archivos/recording.png}
	\centering
	\caption{Example of a running scene being recorded.}
	\label{fig:recording}
\end{figure}

When the sequence finishes, we can stop recording. If we take a look at our designated Scene Folder for the data generation we can se our recording raw data in TXT format. We now need, in order to get it ready for playback, parse it to JSON, we can do this with the \textbf{Generate Sequence Json} utility we have seen in Figure \ref{fig:tracker_settings}.

\subsection{Playback mode}
\label{sec:playback}

With our recording data in JSON format we are now able to play the sequences in the \gls{ue4} editor, but first we will go through some of the configuration settings for the playback mode, they can be seen in Figure \ref{fig:playback_settings} and are further explained below:

\begin{itemize}
	\item \textbf{Json File Names:} Array containing all the JSON filenames that we want to playback.
	\item \textbf{Start Frames:} Array that contains the starting frames for each JSON. The index in this array will directly correlate to the one in the \textbf{Json File Names} array.
	\item \textbf{Playback Only:} When active, it will only play the sequence, skipping the data generation process.
	\item \textbf{Playback Speed Rate:} As its own name indicates, allows to set the speed of the playback, although it can only be used in \textbf{Playback Only} mode. 
	\item \textbf{Generate RGB, Depth, Object Mask, Normal:} It will generate the RGB (format can be adjusted in the \textbf{Format RGB} option), Depth, Segmentation Masks and Normal maps for each frame and camera.
	\item \textbf{Generate Depth Txt Cm:} Generates an equivalent TXT file to the Depth image, where the depth values are stored as plain text.
	\item \textbf{Screenshot Save Directory:} Base path where the Screenshot folder will be located.
	\item \textbf{Screenshot Folder:} Name of the screenshot folder.
	\item \textbf{Width/Height:} Output resolution for the generated images.
\end{itemize}

\begin{figure}[h]
	\includegraphics[width=0.5\textwidth]{archivos/playback_settings.png}
	\centering
	\caption{ROXTracker playback settings.}
	\label{fig:playback_settings}
\end{figure}

Once the desired configuration is set and the Record Mode is disabled , we can hit play in the editor. The ROXTracker object will start parsing the sequence JSON file and generating the output images in our designated folder, an example of the four different outputs is shown in Figure \ref{fig:playback_output}.

\begin{figure*}
	\centering
	\begin{subfigure}[b]{0.475\textwidth}
		\centering
		\includegraphics[width=\textwidth]{archivos/generated_rgb.jpg}
		\caption[Output RGB image.]%
		{{\small Output RGB image.}}    
		\label{fig:generated_rgb}
	\end{subfigure}
	\hfill
	\begin{subfigure}[b]{0.475\textwidth}  
		\centering 
		\includegraphics[width=\textwidth]{archivos/generated_mask.png}
		\caption[Output Segmentation Masks.]%
		{{\small Output Segmentation Masks.}}    
		\label{fig:generated_mask}
	\end{subfigure}
	\vskip\baselineskip
	\begin{subfigure}[b]{0.475\textwidth}   
		\centering 
		\includegraphics[width=\textwidth]{archivos/generated_depth.png}
		\caption[Output Depth image.]%
		{{\small Output Depth image.}}    
		\label{fig:generated_depth}
	\end{subfigure}
	\quad
	\begin{subfigure}[b]{0.475\textwidth}   
		\centering 
		\includegraphics[width=\textwidth]{archivos/generated_normals.png}
		\caption[Output Normals image.]%
		{{\small Output Normals image.}}    
		\label{fig:generated_normals}
	\end{subfigure}
	\caption{Output examples of the Tracker in playback mode.}
	\label{fig:playback_output}
\end{figure*}

\section{Implementing a SegNet using PyTorch}
\label{sec:segnet}
As we previously mentioned, one of the main goals of this work is to study how synthetic data can help semantic segmentation algorithms. For this purpose, a SegNet has been implemented and trained with a real-world human-pose dataset, this section will cover all the data processing, as well as the design and development process of such network.

\subsection{Preprocessing the dataset}
\label{sec:preprocess}
In section \ref{sec:datasets} we mentioned a few of the most important datasets in the field, however, all of them are general purpose oriented, and for this work we needed a human pose dataset. Because of this, we decided to use the \gls{utp} \cite{} dataset. Most of the images from this dataset come from the MPII Human Pose Dataset and contains both the RGB and the Segmentation Mask image.

However, some data pre-processing will be needed in order to fit the data to our needs. 

\subsubsection{Merging the segmentation masks}
The \gls{utp} dataset is divided by segmentation instances. This means that a full image with different persons is divided in different images, each with its unique segmentation mask. For our purpose, we wanted the full image without the instance information. The dataset provides a CSV file which contains the image ID for every segmentation instance in sequential order, this means that we can get all the instances that share the same image ID by iterating through the CSV file, Listing \ref{code:numInstances} shows how we obtain the amount of instances for a single image ID.

\begin{lstlisting}[style=Python-color, caption=Obtaining the number of instances for a single image, frame=single, label=code:numInstances]
	csv = pd.read_csv(csv_path)
	
	while x < len(csv):
		instances = 0
		
		for id in range(x, len(csv)):
			if csv['mpii_id'][id] == csv['mpii_id'][id+1]:
				instances += 1
			else:
				break
\end{lstlisting}

With the amount of instances and having our iterator $x$ pointing to the first image we can now obtain all the following images and merge them as shown in Listing \ref{code:mergeInstances}. To do this, we use the $Paste$ method from the $Pillow$ library, this method takes two inputs, the image to be pasted and the mask which contains the pixels that are to be copied into the our first image. In our case the mask already fits our purpose since it is the mask that we want to copy.

\begin{lstlisting}[style=Python-color, caption=Merging the instance masks into a single image, frame=single, label=code:mergeInstances]
	first_image = Image.open(img_path + str(x))
	
	for n in range(1, instances + 1):
		next_image = Image.open(img_path + str(x + n))
		first_image.paste(next_image, (0,0), next_image.convert('L'))
		
	new_id += 1
	x += instances + 1
\end{lstlisting}

\subsubsection{Creating the dataset class}
Before we can train our network, we need to design a data loader class that will fetch our dataset and transform the data to the proper format. For this purpose we used the PyTorch Dataset class, this way we will be able to create our data loaders and all the data and batch processing will be handled by the framework. Listing \ref{code:UTPDataset} shows the main structure of the UTPDataset class.

\begin{lstlisting}[style=Python-color, caption=UTPDataset definition, frame=single, label=code:UTPDataset]
	class UTPDataset(Dataset):
		def __init__(self, img_dir, transform=None):
			self.transform = transform
			self.image_root_dir = img_dir
			self.img_extension = '_full.png'
			self.mask_extension = '_segmentation_full.png'
			
		def __getitem__(self, index):
			image_id = str(index).zfill(5)
			image_path = os.path.join(self.image_root_dir, image_id, self.img_extension)
			mask_path = os.path.join(self.image_root_dir, image_id, self.mask_extension)
			
			image = self.load_image(path=image_path)
			mask = self.load_mask(path=mask_path)
			
			data =  {
						'image': torch.FloatTensor(image),
						'mask' : torch.LongTensor(mask)
					}
					
			return data
			
\end{lstlisting}

The \textbf{$\_\_getitem\_\_$} method will return the rgb-mask pair when given an index. In order to load the image, we compute the filename using the $zfill$ method, this will add leading zeros to the index so it fits the image name. We then call the $load\_image$ and $load\_mask$ methods shown in Listing \ref{code:UTPload} which will load and preprocess the data. Finally we insert it into a small dictionary and return it.

\begin{lstlisting}[style=Python-color, caption=UTPDataset rgb and mask load and pre-processing, frame=single, label=code:UTPload]
	PALETTE = {
		(0,		0,		0)		:	0,		#Human
		(255,	255,	255)	:	1,		#Background
	}

	def load_image(self, path=None):
		raw_image = Image.open(path)
		raw_image = np.transpose(raw_image.resize((224,224)), (2,1,0))
		imx_t = np.array(raw_image, dtype=np.float32)/255.0
		
		return imx_t

	def load_mask(self, path=None):
		raw_image = Image.open(path)
		raw_image = raw_image.resize((224,224))
		imx_t = np.array(raw_image)
		label_seg = np.zeros((2,224,224), dtype=np.int)
		
		for k in PALETTE:
			label_seg[PALETTE[k]][(imx_t==k).all(axis=2)] = 1
		
		return label_seg
		
\end{lstlisting}

It is important to remark that the input data format for the masks is $(N \times C \times H \times W)$ where $N$ is the batch size, $C$ the number of classes and $H \times W$ the height and width of the image. The segmentation masks are one-hot encoded, which means each class has a $(1 \times H \times W)$ image where the pixels belonging to the $C$ class are stored as 1 and the rest as 0. The method $load\_mask$ in Listing \ref{code:UTPload} performs such encoding, we iterate through all the classes stored on the $PALLETE$ dictionary, where the RGB values for each class are stored. For every class and starting with a zero-filled matrix, we write only on the pixel coordinates of the mask that corresponds to the RGB value of the current class. This encoding is depicted in Figure \ref{fig:onehot}.

\begin{figure}[h]
	\centering
	
	\begin{subfigure}[b]{0.7\textwidth}
		\includegraphics[width=1\linewidth]{archivos/segmentation_encoding.png}
		\caption{Regular segmentation mask codification.}
	\end{subfigure}
	
	\begin{subfigure}[b]{0.5\textwidth}
		\includegraphics[width=1\linewidth]{archivos/segmentation_encoding2.png}
		\caption{One-hot encoded masks.}
	\end{subfigure}
	
	\caption[One-hot encoding format from a regular segmentation mask.]{One-hot encoding format from a regular segmentation mask. Extracted from Jeremy Jordan semantic segmentation post\footnotemark.}
	\label{fig:onehot}
\end{figure}
\footnotetext{\url{https://www.jeremyjordan.me/semantic-segmentation/}}

Also, when loading images from the UnrealROX dataset, segmentation masks are slightly different since we have more than 30 classes. Listing \ref{code:rox-onehot} shows a small script that encodes the segmentation masks so that it only contains the human and background class. Result can be seen in Figure \ref{fig:rox-onehot}.

\begin{lstlisting}[style=Python-color, caption=Preprocessing the UnrealROX segmentation masks, frame=single, label=code:rox-onehot]
imx_t = np.array(raw_image)
imx_t = imx_t[:,:,:3]
label_seg = np.zeros((2,224,224), dtype=np.int)

label_seg[0][(imx_t==[255,0,0]).all(axis=2)] = 1
label_seg[1][np.where(label_seg[0] == 0)] = 1

\end{lstlisting}

It works similarly to the one-hot encoding on the $load\_mask$ method on Listing \ref{code:UTPload}, we create a $(2 \times H \times W)$ zero-filled array. Then for the first layer we fill with ones the [255,0,0] RGB value since it corresponds to the human class, then for the second layer we simply invert the first layer to obtain the background.

\begin{figure}[h]
	\centering
	\begin{subfigure}[b]{0.4\textwidth}
		\includegraphics[width=1\linewidth]{archivos/rox_mask.png}
			\caption{Sample UnrealROX segmentation mask.}
	\end{subfigure}
	
	\begin{subfigure}[b]{0.6\textwidth}
		\includegraphics[width=1\linewidth]{archivos/onehot_mask.png}
		\caption{One-hot two-class segmentation mask.}
	\end{subfigure}
	
	\caption{UnrealROX segmentation masks before and after the pre-process pass.}
	\label{fig:rox-onehot}
\end{figure}

\subsection{Training the network}
\label{sec:training}
In this Subsection we will go through the network implementation (Subsection \ref{sec:model}) and the training script (Subsection \ref{sec:train_script}).

\subsubsection{SegNet Model}
\label{sec:model}
In Section \ref{sec:segmentation_variants} we described the encoder-decoder variant used on \gls{cnn}s for semantic segmentation, specifically, we described the SegNet architecture, to sum it up, the encoder part is exactly the same as the VGG-16 shown in Figure \ref{fig:vgg16} while the decoder performs the reverse up-convolution as de decoder. This Subsection describes the model implementation we used\footnote{\url{https://github.com/Sayan98/pytorch-segnet/blob/master/src/model.py}} for this work.

\begin{figure}[h]
	\includegraphics[width=0.6\textwidth]{archivos/vgg16.png}
	\centering
	\caption{Illustration of the VGG-16 architecture.}
	\label{fig:vgg16}
\end{figure}

The encoder and decoder layout equivalent in PyTorch is shown in Listings \ref{code:encoder} and \ref{code:decoder}. The decoder fundamentally consists of convolutional layers followed by a batch normalization. At the decoder, the convolutions are replaced by Convolutional Transposed layer, which applies transposed convolution that upsamples the outputs, batch normalization is also applied in every layer.

\begin{lstlisting}[style=Python-color, caption=First layers of the SegNet encoder, frame=single, label=code:encoder]
self.encoder_conv_00 = nn.Sequential(*[nn.Conv2d(in_channels=self.input_channels, out_channels=64, kernel_size=3, padding=1), nn.BatchNorm2d(64) ])

self.encoder_conv_01 = nn.Sequential(*[ nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1), nn.BatchNorm2d(64)])

...

self.encoder_conv_42 = nn.Sequential(*[nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1), nn.BatchNorm2d(512)])
\end{lstlisting}

\begin{lstlisting}[style=Python-color, caption=First layers of the SegNet decoder, frame=single, label=code:decoder]
self.decoder_convtr_42 = nn.Sequential(*[nn.ConvTranspose2d(in_channels=512, out_channels=512, kernel_size=3, padding=1), nn.BatchNorm2d(512)])

self.decoder_convtr_41 = nn.Sequential(*[nn.ConvTranspose2d(in_channels=512, out_channels=512, kernel_size=3, padding=1), nn.BatchNorm2d(512)])

...

self.decoder_convtr_00 = nn.Sequential(*[nn.ConvTranspose2d(in_channels=64, out_channels=self.output_channels, kernel_size=3, padding=1)])

\end{lstlisting}

Then, the forward function of the model is shown in Listings \ref{code:encoder_forward} and \ref{code:decoder_forward}. On the decoder, as with every convolutional network, we apply the activation function between layers and pooling at the end of the convolution. In the decoder, however, the order is reversed, we first apply the reverse pooling function or unpooling, then the activation functions and, finally, in the output layer, we apply a softmax classifier in order to get pixel-wise predictions.

\begin{lstlisting}[style=Python-color, caption=Forward function on the encoder, frame=single, label=code:encoder_forward]
dim_0 = input_img.size()
x_00 = F.relu(self.encoder_conv_00(input_img))
x_01 = F.relu(self.encoder_conv_01(x_00))
x_0, indices_0 = F.max_pool2d(x_01, kernel_size=2, stride=2, return_indices=True)

...

dim_4 = x_3.size()
x_40 = F.relu(self.encoder_conv_40(x_3))
x_41 = F.relu(self.encoder_conv_41(x_40))
x_42 = F.relu(self.encoder_conv_42(x_41))
x_4, indices_4 = F.max_pool2d(x_42, kernel_size=2, stride=2, return_indices=True)

\end{lstlisting}

\begin{lstlisting}[style=Python-color, caption=Forward function on the decoder, frame=single, label=code:decoder_forward]
x_4d = F.max_unpool2d(x_4, indices_4, kernel_size=2, stride=2, output_size=dim_4)
x_42d = F.relu(self.decoder_convtr_42(x_4d))
x_41d = F.relu(self.decoder_convtr_41(x_42d))
x_40d = F.relu(self.decoder_convtr_40(x_41d))
dim_4d = x_40d.size()

...

x_0d = F.max_unpool2d(x_10d, indices_0, kernel_size=2, stride=2, output_size=dim_0)
x_01d = F.relu(self.decoder_convtr_01(x_0d))
x_00d = self.decoder_convtr_00(x_01d)
dim_0d = x_00d.size()

x_softmax = F.softmax(x_00d, dim=1)

\end{lstlisting}

\subsubsection{Training script}
\label{sec:train_script}

In order to train our model, a small training script was built. To do this, we first need to create our data loaders and split the data into train and validation splits. When training, we can not only use the training loss as a metric, since the network can over-fit the training samples, meaning that it will not be good at generalization. Listing \ref{code:dataloaders} shows how the PyTorch DataLoader\footnote{\url{https://pytorch.org/docs/stable/data.html}} class can be use to create a data loader.

\begin{lstlisting}[style=Python-color, caption=Data loaders and train-val split, frame=single, label=code:dataloaders]
full_dataset = UTPDataset(img_dir='dataset/dataset')

train_size = int(0.8 * len(full_dataset))
val_size = len(full_dataset) - train_size

train_dataset, val_dataset = torch.utils.data.random_split(full_dataset, [train_size, val_size])

train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=6)
val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=6)

data_loaders = {"train": train_dataloader, "val": val_dataloader}

\end{lstlisting}

In order to create two different data loaders for training and validation, we split the dataset with the $random\_split$ function. Then we create the two data loaders and insert them into a dictionary, this will allow us to easily differentiate the phase in every epoch.

\begin{lstlisting}[style=Python-color, caption=Model criterion and optimizer definition, frame=single, label=code:optimizer]
model = SegNet(input_channels=3, output_channels=2).cuda()
criterion = torch.nn.CrossEntropyLoss().cuda()
optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)

\end{lstlisting}

Listing \ref{code:optimizer} shows the definition of our model, the criterion, which is the metric that will be used to determine how far our predictions are from the ground-truth annotation, in our case, we used Cross Entropy Loss. Finally we define our optimizer, which will be handling how we modify the network weights based on different criteria, in our case, we used the Adam optimizer.

With all of the initialization ready, we can now start training, Algorithm \ref{alg:train} shows a simplified version of the training script. For every epoch, we will process the whole dataset, which is divided in two different loaders, the $phase$ loop will determine which phase is currently being processed and we change our model to the proper phase using the $train$ and $eval$ methods, then the inner most loop will iterate through all the batches of the loader. In this inner loop, we obtain the input RGB images and the target segmentation masks, perform the forward propagation or prediction and then the loss is computed based on our prediction and the desired one. It is important to remark that if we are in the training phase, we also need to compute the gradients based on our loss, as well as update the weights using our optimizer, these two steps are not needed in the validation phase since we are just doing it as a metric to know how the network is performing during training.\newline

\begin{algorithm}[H]
	\DontPrintSemicolon
	\For{epoch < EPOCHS}{
		\For{phase \textbf{in} ['train', 'val']}{
			\eIf{phase == 'train'}{ model.train() }{ model.eval() }
			\For{batch \textbf{in} data\_loaders[phase]}{
				input, target = batch['image'], batch['mask']
				
				output = model(input)\;
				loss = criterion(output, target)
				
				\If{phase == 'train'}{
					loss.backward()\;
					optimizer.step()
				}
				running\_loss += loss
			}
		}
	}
	\caption{Training script}
	\label{alg:train}
\end{algorithm}

In order to prevent our model to overfit, model checkpoints were implemented. This method, although very simple, allows us to keep the best model obtained during training. Listing \ref{code:checkpoints} shows this implementation, where $prev\_loss$ is our best loss in the validation pass and $running\_loss$ our current validation loss. This is computed after every epoch. The $torch.save$ method will write to disk the $mode.state\_dict()$ which is a dictionary containing the model's learnable parameters (weights and biases).


\begin{lstlisting}[style=Python-color, caption=Model checkpoints, frame=single, label=code:checkpoints]
if running_loss < prev_loss:
	torch.save(model.state_dict(), os.path.join('.', 'epoch-{}.pth'.format(epoch+1)))
	prev_loss = running_loss

\end{lstlisting}

\subsubsection{Loading a trained model}
In order to load the model once it has been trained, PyTorch provides the $load\_state\_dict$ method, Listing \ref{code:load_model} shows an example usage. It is important to note that this will only work for inference and not for training, since there are multiple parameters like the optimizer or the loss metrics that are not saved on the $state\_dict$, however, PyTorch does provide with a custom $save$ method that allows to save a custom $dict$ and thus allows to save all the necessary parameters so that it can be used to resume training.

\begin{lstlisting}[style=Python-color, caption=Load model checkpoint, frame=single, label=code:load_model]
model = SegNet(input_channels=3, output_channels=2)
model.load_state_dict(torch.load(PATH))
model.eval()

\end{lstlisting}