%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Plantilla TFG/TFM
% Escuela Polit√©cnica Superior de la Universidad de Alicante
% Realizado por: Jose Manuel Requena Plens
% Contacto: info@jmrplens.com / Telegram:@jmrplens
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{State of the Art}
\label{marcoteorico}
\textit{Semantic segmentation is an extremely important task in the field of computer vision due to its enormous value towards complete scene understanding. Because of this, many works on this matter have been published. In this chapter we analyze some of the most relevant ones and it is organized as follows: Section \ref{sec:intro} give a brief introduction to the Semantic segmentation problem. In section \ref{sec:sim2real} we delineate the importance of the Sim-To-Real field, as well as review some of the latest works on the matter. In Section \ref{sec:architectures} we cover several of the most important and recent deep network architectures. Finally in Section \ref{sec:datasets} we take a look at some of the most important data-sets and frameworks that tackle the semantic segmentation problem. }

\section{Introduction}
\label{sec:intro}
Before we dive into the next sections it is important to understand the semantic segmentation problem and where it comes from. Semantic segmentation is a natural evolution of the object recognition problem, the goal is to infer the class for every pixel on the image, obtaining a pixel-by-pixel labeled output. 
Semantic segmentation is not so different from classic object recognition, in fact it is fundamentally the same, it just adds an extra layer of complexity towards a more fine-grained solution. With semantic segmentation, not only we are able to infer what are the objects in a certain scene, but we also gain knowledge of its localization and exact boundaries. We could go further and try to differentiate instances of the same class, that would be instance segmentation, Figure \ref{fig:object_recognition} shows the different object recognition solutions from less to more complex.

\begin{figure} [h]
	\centering
	\begin{tabular}{cccc}
		\includegraphics[width=0.223\textwidth]{archivos/cat_classification.png} &
		\includegraphics[width=0.223\textwidth]{archivos/cat_localization.png} &
		\includegraphics[width=0.4\textwidth]{archivos/cats_localization.png} \\
		\textbf{(a)}  & \textbf{(b)} & \textbf{(c)}  \\[6pt]
	\end{tabular}
	\begin{tabular}{cccc}
		\includegraphics[width=0.4\textwidth]{archivos/cats_semantic_segmentation.png} &
		\includegraphics[width=0.4\textwidth]{archivos/cats_instance_segmentation.png} \\
		\textbf{(d)}  & \textbf{(e)}  \\[6pt]
	\end{tabular}
	\caption{ \textbf{(a)} Object detection
		\textbf{(b)} Object localization
		\textbf{(c)} Multiple object localization
		\textbf{(d)} Semantic segmentation
		\textbf{(e)} Instance segmentation.}
	\label{fig:object_recognition}
\end{figure}

\section{Sim 2 Real}
\label{sec:sim2real}
In the last decade, data driven algorithms have vastly surpassed traditional techniques for computer vision problems \citep{inbook}. These algorithms, although can be tuned and improved in many different ways, still require vast amounts of quality, precisely annotated data in order to yield good results. In the real world environment, there are quite a few limitations to the quantity and quality of the data that can be produced. For instance, we could be limited to the number of cameras and annotators, moving physical objects to setup scenes could also be difficult and time consuming, and dangerous situations could be risky to set up properly, i.e., trying to get an autonomous car to learn to avoid pedestrians when there is no time to brake.
    
Sim2Real is a specific section of the data science field that mainly focuses on the automatic generation and ground-truth annotation of synthetic data by simulating the real world in a virtual environment. Although a virtual environment allows us to workaround the previously mentioned restrictions, there is still a reality gap that must be covered in order for the synthetic data to be transferred to real life situations. Most synthetic data scenarios present discrepancies between them and the real world, to overcome this and properly transfer the knowledge to real problems there are two known approaches that have been proven to be effective: 

\begin{itemize}
	\item \textbf{Photorealism:} This is the most intuitive approach and the main idea behind it is to simply generate extremely realistic environments as close to reality as possible. To achieve this, multiple techniques can be applied and the current state of the art in this field has grown substantially in the last decade. Such techniques include rendering very high and photo-realistic textures, models and lightning or simulating the noise of real cameras by applying filters and post-process effects. 
	
	One of the most recent and promising innovations is real-time ray tracing, which is a technique that substitutes the traditional rasterization step of the classic rendering pipeline, Figure \ref{fig:pipeline} illustrates both pipelines. Instead of discretizing the scene and assigning the pixel values, ray tracing simulates the behavior of the lightning by casting (tracing) the path of light as pixels, simulating effects such as reflection, refraction and scattering. This allows for higher precision in the pixel values since it takes into account all of the materials of the scene where the light bounced.
	
	\begin{figure}
		\centering
		\includegraphics[width=\linewidth]{archivos/pipeline}
		\caption[Traditional rasterization pipeline in contrast to the ray tracing pipeline.]{Traditional rasterization pipeline in contrast to the ray tracing pipeline. Extracted from NVIDIA devblog\footnotemark.}
		\label{fig:pipeline}
	\end{figure}
	\footnotetext{\url{https://devblogs.nvidia.com/vulkan-raytracing}}
	
	Normally, ray tracing techniques are performed offline since they are quite heavy in terms of computation times. However, the recent RTX NVIDIA Graphic cards \footnote{\url{https://www.nvidia.com/es-es/geforce/20-series/}} feature a new type of processing unit, the RT cores, which specialize in ray tracing computing and are able to accelerate such process, allowing for real time ray tracing. Although this technique is still relatively recent, it is very promising.

	\item \textbf{Domain randomization:} This is the main alternative to photorealism \citep{DBLP:journals/corr/TobinFRSZA17} when trying to cover the reality gap between the real world and the synthetic environments. This technique is based on feeding the model a large amount of randomized variations of certain synthetic object or environment. This is done by randomizing certain inputs like the materials, lightning, animations or meshes. With this method even if the data is not represented with extreme fidelity, the variability of the multiple range of slightly different samples will make up for it. With enough randomization, the real world sample will appear as just another variation, which will allow the model to generalize. Figure \ref{fig:domainrandomization} shows an example of randomized training inputs.
	
	\begin{figure}
		\centering
		\includegraphics[width=0.5\linewidth]{archivos/domain_randomization}
		\caption{Low-fidelity images with random variations in camera angle, lightning and positions are used to train an object detector. Testing is done in the real world. Image from \cite{DBLP:journals/corr/TobinFRSZA17}.}
		\label{fig:domainrandomization}
	\end{figure}
	
	\item \textbf{Domain Adaptation:} This is another interesting approach to reduce the reality gap and was presented by \cite{DBLP:journals/corr/abs-1812-05040}. The main takeaway behind this idea is to transfer the real-world style of images into the synthetic ones. This is done by an architecture based on the \gls{gan} (see Figure \ref{fig:gan}). First, the \textit{Image Transform Network} outputs a new generated image by using as inputs the synthetic RGB image, the segmentation mask and also the depth map. The \textit{Image Discriminator} has to differentiate between the real image and the one generated by the network. 
	
	Then, the \textit{Task Network} performs a second adaptation pass at the output level, predicting the outputs of both real and synthetic images. A second discriminator has to discern if the outputs are predicted from a real or the synthetic adapted image. Figure \ref{fig:gan} shows their proposed architecture.
	
	\begin{figure}
		\centering
		\includegraphics[width=\linewidth]{archivos/gan}
		\caption{Overview of the Input-Output adaptation network form \cite{DBLP:journals/corr/abs-1812-05040}}
		\label{fig:gan}
	\end{figure}
 
\end{itemize}

\subsection{VirtualHome}
VirtualHome by \cite{virtualhome2018} is a three-dimensional environment built in the Unity game engine. The main goal is to model complex tasks in a household environment as sequences of more atomic and simple instructions.
 
In order to perform this task, a big database describing activities composed by multiple atomic steps is necessary, as in the human natural language there is a lot of information that is common knowledge and is usually omitted, however, for a robot or agent this information has to be provided in order to fully understand a task. For this purpose an interface to formalize this tasks was built on top of the Scratch \footnote{\url{https://scratch.mit.edu/}} MIT project. Then all of this atomic actions and interactions were implemented using the Unity3D game engine.

For the data collection, they crowdsourced the natural language description of these tasks and then built them using the Scratch interface. Every task is composed by a sequence of steps where every step is a Scratch block, and every block defines a syntactic frame and a list of arguments for the different interactions that they may have.

Every step $t$ in the program can be written as:

\[ step_t = [action_t] (object_{t,1})(id_{t,1}) ... (object_{t,n})(id_{t,n}) \]

Where $id$ is an identifier to differentiate instances of the same object. An example program to "watch tv" would look like this: 
\newline

$step_1$ = [Walk] (TELEVISION)(1)

$step_2$ = [SwitchOn] (TELEVISION)(1)

$step_3$ = [Walk] (SOFA)(1)

$step_4$ = [Sit] (SOFA)(1)

$step_5$ = [Watch] (TELEVISION)(1) \newline

Another example this time with the scratch block interface can be seen in figure \ref{fig:virtualhome_example}.

\begin{figure}[h]
	\includegraphics[scale=0.5]{archivos/virtualhome_example.png}
	\centering
	\caption{List of actions represented with the scratch interface, where the user can manually add, modify and change the arguments of every action.}
	\label{fig:virtualhome_example}
\end{figure}

\subsection{UnrealROX}
UnrealROX by \cite{DBLP:journals/corr/abs-1810-06936} is a photo-realistic 3D virtual environment built in \gls{ue4} capable of generating synthetic, ground-truth annotated data. Unlike VirtualHome, the main method to record sequences is to actually control the actors manually making use of the \gls{vr} headset and controllers, this will be further explained in the following chapters.

\section{Common Architectures}
\label{sec:architectures}
As we previously stated, semantic segmentation is a natural step towards the more fine-grained image recognition problem, since the information that we are trying to infer is of a higher level, we also require more complex architectures. Although the models we review in this section work properly for image recognition and detection, some modifications have to be made in order to adapt them for segmentation problems. However, they have made such significant contributions to the field that they are still used as the basic building blocks of segmentation architectures.

\subsection{AlexNet}
AlexNet by \cite{AlexNet} was the first deep network architecture that successfully surpassed traditional machine learning approaches, winning the \gls{ilsvrc} 2012 with a 84.6\% TOP-5 test accuracy, surpassing its competitors by a considerable margin. The architecture itself consists of five convolution + pooling layers followed by three fully connected ones as seen in Figure \ref{alexnet}.

\begin{figure}[h]
	\includegraphics[scale=0.3]{archivos/alexnet.png}
	\centering
	\caption{AlexNet architecture reproduced from \cite{AlexNet}}
	\label{alexnet}
\end{figure}

\subsection{VGG}
VGG by \cite{DBLP:journals/corr/SimonyanZ14a} is also a deep network model introduced by the \gls{vgg}, one of the various model configurations proposed was submitted to the \gls{ilsvrc} 2013. VGG-16 achieved 92.7\% TOP-5 test accuracy.

The structure of VGG-16, it consists of 16 convolutional layers, and just like AlexNet, uses three fully connected layers for classification. The main improvement over AlexNet was made substituting the first large kernel sizes in the first few layers with multiple 3x3 sequential kernel filters.

\subsection{GoogLeNet}
GoogLeNet (also known as Inception) was introduced by \cite{DBLP:journals/corr/SzegedyLJSRAEVR14} and was submitted to \gls{ilsvrc} 2014, winning with a TOP-5 test accuracy of 93.3\%. The architecture of GoogLeNet introduced the inception module (shown in Figure \ref{inception}) which is a new approach where convolution layers were not stacked in just sequential order but instead had some of them computed in parallel, which substantially reduced computational cost. The outputs of the different layers where then concatenated and moved towards the next module.

\begin{figure}
	\includegraphics[scale=0.3]{archivos/inception_module.png}
	\centering
	\caption{Inception module extracted from \cite{DBLP:journals/corr/SzegedyLJSRAEVR14}}
	\label{inception}
\end{figure}

Ever since their first version, Inception v1, they have been constantly releasing new iterations of the network with constant performance improvements, up to their last Inception v4 release.

\subsection{ResNet}
ResNet by \cite{DBLP:journals/corr/HeZRS15} was first introduced by Microsoft research and it received increasing attention after winning \gls{ilsvrc} 2015 with a TOP-5 test accuracy rate of 96.4\%. This \gls{cnn} has 152 layers (although shallower variations do exist) and it introduced a new concept called residual blocks. Its great number of layers makes the network more prone to the vanishing gradient problem (the backpropagated gradients gets infinitely small and the network performance falls off). The new residual module allowed the network inputs to skip layers and copy the values onto deeper layers, in a way that the computed output is a combination of both the skipped inputs and the forward propagated ones (see Figure \ref{fig:residual}). This helps to reduce the vanishing gradient problem in deep networks and allows each layer with a residual input to learn something new since the inputs are both the encoded values from the previous layer as well as the unchanged inputs.

\begin{figure}
	\includegraphics[width=0.5\textwidth]{archivos/residual.png}
	\centering
	\caption{Residual block extracted from \cite{DBLP:journals/corr/HeZRS15}}
	\label{fig:residual}
\end{figure}

\subsection{ReNet}
Multi Dimensional Recurrent Neural Network (MDRNN) by \cite{DBLP:journals/corr/abs-0705-2011} are a variation of regular Recurrent Neural Networks (RNN) that allows them to work with $d$ spatio-temporal dimensions of the data. However, the architecture proposed by \cite{DBLP:journals/corr/VisinKCMCB15} used regular RNNs instead of MDRNN, the main intuition behind their proposal is that every convolution + pooling layer is replaced by four RNNs that sweep the image across in four directions.

\subsection{Semantic Segmentation Methods}
\label{sec:segmentation_variants}
All the image recognition approaches are based on convolutional architectures, whether it is recognition, detection, localization or segmentation, they all share a big common module, which is the convolution layers that extract the features of any image, then the feature maps can be applied to any classification network structure depending on the desired output format.

Today, almost every semantic segmentation architecture uses the \gls{fcn} by \cite{DBLP:journals/corr/LongSD14}. The idea behind this is to replace the classic fully connected layers of \gls{cnn}s with \gls{fcn}s in order to obtain a spatial map instead of classification outputs, this way we can obtain pixel-wise classification while still using the inferred knowledge and power of the \gls{cnn}s to extract the features. However, a new problem arises when using \gls{cnn}s for semantic segmentation, since convolutional layers (convolution + pooling) down-sample the input image in order to learn features. This down-sampling does not matter when applied to classification problems, however, when doing pixel-wise classification, we require the output image to have the same size as the input. To overcome this problem, spatial maps are then up-sampled by using deconvolution layers as shown in \cite{deconvolution}.

\subsubsection{Decoder Variant}
The decoder variant is another method to adapt networks that were initially made for classification. In this variant, the network after removing the fully connected layers is normally called encoder and it outputs a low-resolution feature map. The second part of this variant is called decoder and the main idea behind it is to up-sample those feature maps to obtain a full resolution pixel-wise classification.

One of the most known examples of this encoder-decoder architecture is SegNet by \cite{DBLP:journals/corr/BadrinarayananK15}, the encoder part is fundamentally the same as a \gls{vgg}-16 without the fully connected layers at the very end, while the decoder part consists of a combination of convolution and up-sampling layers that correspond to the max-pooling ones in the encoder, the whole architecture can be seen in Figure \ref{fig:segnet}. SegNet is capable of achieving very good results while being relatively fast, which makes it a good starting point of any semantic segmentation problem.

\begin{figure}[h]
	\includegraphics[scale=0.5]{archivos/segnet.png}
	\centering
	\caption{Segnet architecture graph extracted from \cite{DBLP:journals/corr/BadrinarayananK15}}
	\label{fig:segnet}
\end{figure}

\subsubsection{Dilated Convolutions}
As we previously mentioned, \gls{cnn}s generate significantly reduced spacial feature maps. To overcome this spatial reduction, dilated convolutions (also known as \textit{atrous} convolutions) can be used in order to aggregate multi-scale contextual information without down-scaling.

The dilation rate $l$ controls the up-sampling factor of the filters. That way, a 1-dilated convolution is just a regular convolution where every element has a receptive field of 1x1, in a 2-dilated every element has a 3x3 receptive field, in a 3-dilated every element has a 7x7, this is depicted in Figure \ref{fig:dilatedconv}. This way, the receptive field grows in a exponential way, while the parameters have a linear growth.

\begin{figure}
	\centering
	\includegraphics[width=0.7\linewidth]{archivos/dilated_conv}
	\caption{(a) 1x1 receptive fields, 1-dilated, (b) 3x3 receptive fields, 2-dilated, (c) 7x7 receptive fields, 3-dilated. Figure extracted from \cite{yu2015multiscale}.}
	\label{fig:dilatedconv}
\end{figure}

Some of the most important works that make use of this technique are the aforementioned multi-context aggregation by \cite{yu2015multiscale} and DeepLab by \cite{DBLP:journals/corr/ChenPK0Y16}.

\subsubsection{Conditional Random Fields}
Deep \gls{cnn}s applied to semantic segmentation excel at classification tasks, however they still lack precision when it comes to spacial information and struggle to properly delineate the boundaries of objects. To overcome this, a last post-processing step in order to refine the output can be applied, for instance, \gls{crf}. This technique makes use of both low level pixel interaction as well as the multi-class inference pixel prediction of high level models.

The DeepLab model by \cite{DBLP:journals/corr/ChenPK0Y16} makes use of \gls{crf} to refine their output, an overview of the model can be seen in Figure \ref{fig:deeplab}. 

\begin{figure}
	\centering
	\includegraphics[width=0.7\linewidth]{archivos/deeplab}
	\caption{Illustration of the DeepLab proposed architecture, using a deep \gls{cnn} for pixel-wise classification and a fully connected \gls{crf} to refine the output.}
	\label{fig:deeplab}
\end{figure}

\section{Datasets}
\label{sec:datasets}
In this section we will review some of the most important datasets that are commonly used to train semantic segmentation architectures. 

\subsection{PASCAL}
PASCAL Visual Object Classes by \cite{Everingham15} is one of the most popular 2D datasets for semantic segmentation. The challenge consists of 5 different competitions, 21 ground-truth annotated classes and a private test set to verify the accuracy of submitted models. Also there are a few extensions of this dataset. One of them is PASCAL Context, which provides pixel-level classification for the entire original dataset. Another extension for the PASCAL dataset that is worth mentioning is PASCAL Part, which further decomposes the instances in smaller classes, for instance, a car is decomposed into wheels, chassis, headlights and windows. Figure \ref{fig:pascal_part} shows more examples of different classes.

\begin{figure}[h]
	\includegraphics[scale=0.35]{archivos/pascal_part.png}
	\centering
	\caption{PASCAL Part examples of ground truth annotated parts for different classes.}
	\label{fig:pascal_part}
\end{figure}

\subsection{Semantic Boundaries Dataset}
SBD by \cite{BharathICCV2011} is an extension of the PASCAL dataset that provides semantic segmentation ground-truth annotations for all the images that were not labeled in the original dataset. These annotations contain class, instance and boundaries information. SBD greatly increases the amount of data from the original PASCAL and, because of this, is commonly used for deep learning architectures.

\subsection{Cityscapes}
Cityscapes by \cite{DBLP:journals/corr/CordtsORREBFRS16} is a urban dataset mainly used for instance and semantic segmentation. It contains over 25000 images and 30 different classes that were recorded in 50 cities during different times of the day and year.

\subsection{KITTI}
The KITTI dataset by \cite{Geiger2013IJRR} was recorded from a vehicle on a urban environment. It includes camera RGB images, laser scans, and precise GPS measurements. Despite being very popular for autonomous driving, it does not contain ground-truth annotations for semantic segmentation. To work around this, some researchers manually annotated parts of the dataset to fit their necessities. 

\subsection{COCO}
COCO is yet another image recognition and segmentation dataset by \cite{DBLP:journals/corr/LinMBHPRDZ14} which mainly focuses on everyday scenes and common objects. The dataset contains 91 different classes and a total of 328.000 images and the labeling methods contain both bounding boxes as well as semantic segmentation.